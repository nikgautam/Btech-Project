{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP Assignment 1\n",
    "\n",
    "Name : G Rohit\n",
    "\n",
    "Student ID : 19233292"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oc7hUIIjL6I9"
   },
   "source": [
    "# Task 1\n",
    "\n",
    "Here I have read the data from the file line by line, each text input is then processed. \n",
    "processTweet function is called for each tweet input. This function cleans the text input provided. \n",
    "\n",
    "Here I have also printed the positive samples size and negative samples size. \n",
    "I have also printed vocabulary size of processed data and unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unprocessed Vocab size:  18576\n",
      "Vocab size:  12199\n",
      "Positive Examples:  1911\n",
      "Negative Examples:  1923\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) # list of stopwords to filter from the text\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower() # convert tweet to lower case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet) # remove the http and other links from the tweet\n",
    "    tweet = tweet.replace(\":p\",\"\") # remove the :p smiley from the tweet\n",
    "    tweet = tweet.replace(\"--\",\" \") # remove the sequence '--' from the tweet\n",
    "    tweet= tweet.translate(str.maketrans('', '', string.punctuation)) # remove all puntuations from the tweet\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) # remove all the @ mentions from the tweet\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet) # replace extra spaces from tweet to a single space\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # replace hashtag word with only just word\n",
    "    # remove stopwords from the tweet\n",
    "    word_tokens = word_tokenize(tweet) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    tweet = \" \".join(filtered_sentence)\n",
    "    tweet = tweet.strip('\\'\"') # remove quotes from the starting and ending of the tweet\n",
    "    return tweet\n",
    "\n",
    "file = open(\"SemEval2018-T3-train-taskA.txt\",'rt',encoding='utf-8') # reading dataset\n",
    "header = True\n",
    "data, data_labels  = [], []\n",
    "vocab, unprocessed_vocab = [], []\n",
    "pos_examples = []\n",
    "neg_examples  = []\n",
    "for line in file : \n",
    "    if header : \n",
    "        header = False\n",
    "        continue\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    tweet = processTweet(line[2])\n",
    "    unprocessed_vocab.extend(line[2].split())\n",
    "    vocab.extend(tweet.split())\n",
    "    data.append(tweet)\n",
    "    data_labels.append(int(line[1]))\n",
    "vocab_size = len(set(vocab))\n",
    "print(\"Unprocessed Vocab size: \", len(set(unprocessed_vocab)))\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "print(\"Positive Examples: \", data_labels.count(1))\n",
    "print(\"Negative Examples: \",data_labels.count(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5LivWxOL6JA"
   },
   "source": [
    "# Task 2\n",
    "\n",
    "Here I have used scikit learn's train_test_split to split the dataset. I have used 80% of the data as training data and 20% data as the testing data. I have the used the ratio of 8:2 for splitting the dataset as this will provide us enough data to train the model.\n",
    "\n",
    "Implemented function performance_evaluation. This function prints out the classification report as well as accuracy for a set of predictions. This functions calls scikit learn's functions to print out the classification report and accuracy\n",
    "\n",
    "I have used CountVectorizer function to convert the text to feature vectors. This feature vector is splitted to tran and test data so that it can be used later while creating a log linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#using CountVectorizer method to get feature vector od the dataset\n",
    "countVectorizer = CountVectorizer(analyzer = 'word')\n",
    "vocab_features = countVectorizer.fit_transform(data)\n",
    "feature_vector = vocab_features.toarray()\n",
    "X_train, X_test, y_train, y_test  = train_test_split(feature_vector, data_labels, train_size=0.80, random_state=1)\n",
    "\n",
    "#function to calculate accuracy, precision, recall, f1-score\n",
    "def performance_evaluation(y_true,y_pred):\n",
    "    print('Accuracy:\\t{:0.1f}%'.format(accuracy_score(y_true,y_pred)*100))   \n",
    "    #classification report\n",
    "    print('\\n')\n",
    "    print(classification_report(y_true, y_pred)) # prints precision, recall and f1 score\n",
    "    #confusion matrix\n",
    "    confmat = confusion_matrix(y_true, y_pred)\n",
    "    print(\"The Confusion matrix is: -\")\n",
    "    print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU9HGp4eo_0C"
   },
   "source": [
    "# Task 3\n",
    "\n",
    "Here I have used scikit learn's LogisticRegression model to classify the tweet. This model uses a simple log linear activation function to fit on the dataset.\n",
    "\n",
    "The input to this model is the CountVectorizer feature vector. Predcitions from this model is then sent to performance_evaluation function which was defined earlier. This will print out the classification summary of this model.\n",
    "\n",
    "Accuracy of this model : 60.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t60.6%\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.68      0.62       369\n",
      "           1       0.64      0.54      0.59       398\n",
      "\n",
      "    accuracy                           0.61       767\n",
      "   macro avg       0.61      0.61      0.61       767\n",
      "weighted avg       0.61      0.61      0.60       767\n",
      "\n",
      "The Confusion matrix is: -\n",
      "[[250 119]\n",
      " [183 215]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(solver='lbfgs')\n",
    "log_model = log_model.fit(X=X_train, y=y_train)\n",
    "y_pred = log_model.predict(X_test)\n",
    "performance_evaluation(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82JnhmgBL6JA"
   },
   "source": [
    "# Task 4 \n",
    "\n",
    "Here I have written three functions which will be passed as custom metrics to the Keras model for evalation. These function calculate Precision, Recall, F1-Score of the keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def calc_recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def calc_precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def calc_f1(y_true, y_pred):\n",
    "    precision = calc_precision(y_true, y_pred)\n",
    "    recall = calc_recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am building a Keras model for a simple RNN LSTM model. \n",
    "The input to this model is keras tokenizer. This converts each word in the vocabulary to a unique id. Then the dataset is replaced with the unique ids. After tokenizing the dataset, padding is done with(max length as 20). This will make sure that each tweet is of the same length. Padding is done to starting of the input. This padded input sequence is then again splitted into 8:2 ratio i.e. divided into test and train dataset. Model is trained on the train set. After that the model is tested against the test dataset and accuracy, precision, recall, f1-score is then printed. \n",
    "I have used embedding size as 128. I have also used dropout in the LSTM layer\n",
    "\n",
    "Accuracy of this model : 61.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grohi\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3067 samples, validate on 767 samples\n",
      "Epoch 1/5\n",
      "3067/3067 [==============================] - 8s 3ms/step - loss: 0.6870 - accuracy: 0.5465 - calc_recall: 0.6227 - calc_precision: 0.5692 - calc_f1: 0.5365 - val_loss: 0.6653 - val_accuracy: 0.5671 - val_calc_recall: 0.7777 - val_calc_precision: 0.5368 - val_calc_f1: 0.6291_precisio\n",
      "Epoch 2/5\n",
      "3067/3067 [==============================] - 6s 2ms/step - loss: 0.4602 - accuracy: 0.8053 - calc_recall: 0.8024 - calc_precision: 0.8138 - calc_f1: 0.7993 - val_loss: 0.7094 - val_accuracy: 0.6102 - val_calc_recall: 0.6582 - val_calc_precision: 0.5894 - val_calc_f1: 0.6133\n",
      "Epoch 3/5\n",
      "3067/3067 [==============================] - 6s 2ms/step - loss: 0.1476 - accuracy: 0.9501 - calc_recall: 0.9499 - calc_precision: 0.9510 - calc_f1: 0.9491 - val_loss: 1.1101 - val_accuracy: 0.6206 - val_calc_recall: 0.5791 - val_calc_precision: 0.6159 - val_calc_f1: 0.5873\n",
      "Epoch 4/5\n",
      "3067/3067 [==============================] - 6s 2ms/step - loss: 0.0525 - accuracy: 0.9830 - calc_recall: 0.9845 - calc_precision: 0.9836 - calc_f1: 0.9835 - val_loss: 1.3112 - val_accuracy: 0.6089 - val_calc_recall: 0.5708 - val_calc_precision: 0.6004 - val_calc_f1: 0.5750\n",
      "Epoch 5/5\n",
      "3067/3067 [==============================] - 6s 2ms/step - loss: 0.0252 - accuracy: 0.9938 - calc_recall: 0.9930 - calc_precision: 0.9953 - calc_f1: 0.9940 - val_loss: 1.7803 - val_accuracy: 0.6128 - val_calc_recall: 0.4776 - val_calc_precision: 0.6280 - val_calc_f1: 0.5321\n",
      "767/767 [==============================] - 0s 255us/step\n",
      "\n",
      "Rnn Model accuracy: 61.2777054309845\n",
      "Rnn Model Recall: 0.47762879729270935\n",
      "Rnn Model Precison: 0.6279900670051575\n",
      "Rnn Model F1-score: 0.5320706367492676\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "import numpy as np\n",
    "#creating keras tokenizer \n",
    "keras_tokenizer = Tokenizer()\n",
    "keras_tokenizer.fit_on_texts(data)\n",
    "X = keras_tokenizer.texts_to_sequences(data)\n",
    "#converting data to sequences of same length\n",
    "X = sequence.pad_sequences(X, maxlen=20)\n",
    "y = np.asarray(data_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 1234)\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(vocab_size+1, 128))\n",
    "rnn_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', calc_recall, calc_precision, calc_f1])\n",
    "print('Training on data...')\n",
    "rnn_model.fit(X_train, y_train,batch_size=32,epochs=5,validation_data=(X_test, y_test))\n",
    "score, acc, recall, precision, f1_score = rnn_model.evaluate(X_test, y_test,batch_size=32)\n",
    "print()\n",
    "print('Rnn Model accuracy:', acc*100)\n",
    "print('Rnn Model Recall:', recall)\n",
    "print('Rnn Model Precison:', precision)\n",
    "print('Rnn Model F1-score:', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yZucGZmL6JM"
   },
   "source": [
    "# Task 5 \n",
    "\n",
    "As a part of improvement I am implementing a Convolution 1D model for text classification. The concept of convolution is used for images mainly where spatial features matter. For example nose is always near to an eye in an image of a person. So nose and eye is spatially related to each other. These features are detected very nicely with convolution layer. \n",
    "\n",
    "Simlarly when dealing with Text classification, words which are near to each other are also supposed to relate each other. Concept of n-gram says the words occuring together have some meaning to it. If two word occur together that means they have some relationship between them, this should also be captured as features. Convolution 1D will help us to acheive this as it learns automatically about spatial features between words. \n",
    "\n",
    "With this idea I have a built a model in Kearas. This model uses Conv1D layer. \n",
    "\n",
    "Accuracy of this model : 63.5%\n",
    "\n",
    "The accuracy, precision, recall and F1-Score has increased from simple RNN LSTM model. This shows that this model will be helpful for text classfication tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grohi\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3067 samples, validate on 767 samples\n",
      "Epoch 1/5\n",
      "3067/3067 [==============================] - 5s 2ms/step - loss: 0.6909 - accuracy: 0.5276 - calc_recall: 0.5696 - calc_precision: 0.5571 - calc_f1: 0.4872 - val_loss: 0.6750 - val_accuracy: 0.5854 - val_calc_recall: 0.1945 - val_calc_precision: 0.7933 - val_calc_f1: 0.3052\n",
      "Epoch 2/5\n",
      "3067/3067 [==============================] - 4s 1ms/step - loss: 0.4945 - accuracy: 0.7864 - calc_recall: 0.7727 - calc_precision: 0.8004 - calc_f1: 0.7727 - val_loss: 0.6921 - val_accuracy: 0.6245 - val_calc_recall: 0.6764 - val_calc_precision: 0.6001 - val_calc_f1: 0.6271\n",
      "Epoch 3/5\n",
      "3067/3067 [==============================] - 4s 1ms/step - loss: 0.1194 - accuracy: 0.9619 - calc_recall: 0.9630 - calc_precision: 0.9618 - calc_f1: 0.9610 - val_loss: 0.9708 - val_accuracy: 0.6284 - val_calc_recall: 0.6315 - val_calc_precision: 0.6104 - val_calc_f1: 0.6147\n",
      "Epoch 4/5\n",
      "3067/3067 [==============================] - 4s 1ms/step - loss: 0.0242 - accuracy: 0.9945 - calc_recall: 0.9955 - calc_precision: 0.9937 - calc_f1: 0.9944 - val_loss: 1.2118 - val_accuracy: 0.6297 - val_calc_recall: 0.5780 - val_calc_precision: 0.6319 - val_calc_f1: 0.5947\n",
      "Epoch 5/5\n",
      "3067/3067 [==============================] - 4s 1ms/step - loss: 0.0065 - accuracy: 0.9990 - calc_recall: 0.9987 - calc_precision: 0.9993 - calc_f1: 0.9990 - val_loss: 1.4029 - val_accuracy: 0.6349 - val_calc_recall: 0.6022 - val_calc_precision: 0.6316 - val_calc_f1: 0.6077\n",
      "767/767 [==============================] - 0s 190us/step\n",
      "Conv 1D Model accuracy: 63.494133949279785\n",
      "Conv 1D Model Recall: 0.6021519303321838\n",
      "Conv 1D Model Precison: 0.6316357254981995\n",
      "Conv 1D Model F1-score: 0.6077370047569275\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "#hyper parameters: \n",
    "batch_size = 32\n",
    "embedding_dims = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 5\n",
    "\n",
    "#building the model\n",
    "#input layer\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(vocab_size+1, embedding_dims, input_length=20))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "# create a Conv1D layer and then a plain hidden dense with non-linear activation\n",
    "cnn_model.add(Conv1D(filters,kernel_size, padding='valid', activation='relu', strides=1))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(hidden_dims))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Activation('relu'))\n",
    "\n",
    "#output layer with sigmoid activation\n",
    "cnn_model.add(Dense(1))\n",
    "cnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "cnn_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', calc_recall, calc_precision, calc_f1])\n",
    "#training the model\n",
    "cnn_model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test))\n",
    "#testing\n",
    "score, acc, recall, precision, f1_score = cnn_model.evaluate(X_test, y_test,batch_size=batch_size)\n",
    "print('Conv 1D Model accuracy:', acc*100)\n",
    "print('Conv 1D Model Recall:', recall)\n",
    "print('Conv 1D Model Precison:', precision)\n",
    "print('Conv 1D Model F1-score:', f1_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Advanced Topics in Natural Language Processing - Assignment 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
